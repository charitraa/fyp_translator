{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f1ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q datasets torch matplotlib sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa886399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97458203-971d-4ebd-8b64-92d85ed9b329",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43mload_dataset\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mCohleM/english-to-nepali\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m eng_data = data[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m nep_data = data[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mne\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"CohleM/english-to-nepali\")\n",
    "eng_data = data['train']['en']\n",
    "nep_data = data['train']['ne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718f95f-8aca-4047-a016-44baa5dcd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eng_tokenizer_50k.pkl\", \"rb\") as f:\n",
    "    eng_tok = pickle.load(f)\n",
    "with open(\"nep_tokenizer_50k.pkl\", \"rb\") as f:\n",
    "    nep_tok = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c7510-51f8-44aa-b7f8-e40d836711f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in [1500, 1501, 1502]:\n",
    "    eng_tok.vocab[tok] = b''\n",
    "    nep_tok.vocab[tok] = b''\n",
    "\n",
    "PAD_ID = 1500\n",
    "SOS_ID = 1501\n",
    "EOS_ID = 1502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1adb8a-4e30-4177-8611-67fa48bdf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, output_data):\n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        self.output_data = output_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx], self.output_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa48c-e37a-4895-a76a-d3e5f7f9a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    input_seqs, target_seqs, output_seqs = zip(*batch)\n",
    "    max_len = max(max(len(seq) for seq in input_seqs + target_seqs + output_seqs))\n",
    "    pad_tensor = lambda seq: seq + [PAD_ID] * (max_len - len(seq))\n",
    "    return (\n",
    "        torch.tensor([pad_tensor(s) for s in input_seqs]),\n",
    "        torch.tensor([pad_tensor(s) for s in target_seqs]),\n",
    "        torch.tensor([pad_tensor(s) for s in output_seqs])\n",
    "    )\n",
    "\n",
    "dataset = TranslationDataset(enco_eng_data, deco_nep_data_sos, deco_nep_data_eos)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79142c55-12c6-4362-b2dc-e05d38db8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embd, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embd, heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embd, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embd)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embd)\n",
    "        self.norm2 = nn.LayerNorm(embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm2(x + ff_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e3bec-c546-4426-817b-000aa2c84ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embd, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embd, heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embd, heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embd, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embd)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embd)\n",
    "        self.norm2 = nn.LayerNorm(embd)\n",
    "        self.norm3 = nn.LayerNorm(embd)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        self_attn_out, _ = self.self_attn(x, x, x, attn_mask=torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1).to(x.device))\n",
    "        x = self.norm1(x + self_attn_out)\n",
    "        cross_attn_out, _ = self.cross_attn(x, enc_out, enc_out)\n",
    "        x = self.norm2(x + cross_attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm3(x + ff_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c59a64-d037-40d3-9e5c-22fd71a8024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embd, heads, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embd)\n",
    "        self.pe = nn.Embedding(500, embd)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(embd, heads, dropout) for _ in range(layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(embd, heads, dropout) for _ in range(layers)])\n",
    "        self.ln = nn.LayerNorm(embd)\n",
    "        self.out = nn.Linear(embd, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        seq_len = src.size(1)\n",
    "        pos = torch.arange(seq_len, device=src.device).unsqueeze(0)\n",
    "        src = self.embed(src) + self.pe(pos)\n",
    "        enc_out = self.encoder(src)\n",
    "\n",
    "        tgt_len = tgt.size(1)\n",
    "        pos_t = torch.arange(tgt_len, device=src.device).unsqueeze(0)\n",
    "        tgt = self.embed(tgt) + self.pe(pos_t)\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, enc_out)\n",
    "\n",
    "        return self.out(self.ln(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110c900-4a9a-457b-8d3d-75ad8bbccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranslationModel(vocab_size=1503, embd=256, heads=8, layers=4, dropout=0.1).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt, out in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        src, tgt, out = src.cuda(), tgt.cuda(), out.cuda()\n",
    "        logits = model(src, tgt)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), out.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg = total_loss / len(train_loader)\n",
    "    train_loss.append(avg)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg:.4f}\")\n",
    "\n",
    "    # Save model each epoch\n",
    "    torch.save(model.state_dict(), f\"transformer_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d17a3-f73f-4b33-a8b0-2e41185ed71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, marker='o')\n",
    "plt.title(\"Transformer NMT Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbc593-3646-48ac-a0d7-f6dc86164140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_len=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor([eng_tok.encode(sentence)]).to('cuda')\n",
    "        pos = torch.arange(input_ids.size(1), device='cuda').unsqueeze(0)\n",
    "        enc_out = model.encoder(model.embed(input_ids) + model.pe(pos))\n",
    "\n",
    "        tgt_ids = torch.tensor([[SOS_ID]]).to('cuda')\n",
    "        for _ in range(max_len):\n",
    "            pos_t = torch.arange(tgt_ids.size(1), device='cuda').unsqueeze(0)\n",
    "            x = model.embed(tgt_ids) + model.pe(pos_t)\n",
    "            for layer in model.decoder:\n",
    "                x = layer(x, enc_out)\n",
    "            output = model.out(model.ln(x))\n",
    "            next_token = output[:, -1, :].argmax(dim=-1)\n",
    "            tgt_ids = torch.cat([tgt_ids, next_token.unsqueeze(1)], dim=1)\n",
    "            if next_token.item() == EOS_ID:\n",
    "                break\n",
    "        return nep_tok.decode(tgt_ids[0, 1:-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea4afc-5d08-4f25-b985-907da855acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "refs = []\n",
    "for i in range(50):\n",
    "    pred = translate(eng_data[i])\n",
    "    ref = nep_data[i]\n",
    "    preds.append(pred)\n",
    "    refs.append([ref])\n",
    "print(\"BLEU Score:\", corpus_bleu(preds, refs).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5fb5f-22fb-497f-937d-460d4119fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
